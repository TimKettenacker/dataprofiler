## Data Profiling

The reasoning behind having a data profiling is twofold: first, it aims at assessing the impact that unknown datasets may have on your MDM system. At the beginning of any data-related project, you usually don't know how exactly the data from your customer looks like (and neither does the customer). Getting to know the data and its inherent quality is always crucial in an MDM project, as the supplier of an MDM system is always at risk of loading data that is not according to defined standards or simply, in a real bad shape. Failing to assess data quality at an early stage might as well render the purpose of an MDM system useless (garbage in, garbage out). On top of that, matching & deduplication software is usually sensitive to certain triggers that may appear in the data; some matching components conduct analytical steps for themselves, i.e. to identify tokens that contain legal forms in the data. It is important to know how such tokens interact with the matching software to prevent unwanted side effects.

Secondly, it is also about communicating these findings to the customer. Usually, the customer is not aware of the condition of his data. By providing a concise and reproducible examination of his data, the customer will start to build confidence in your skills and learn to perceive you as a trusted advisor in all data-related matters. Raising awareness for the costs of poorly managed data may also spark a change in his mindset and open up possibilities for future data governance initiatives.

The present data profiling toolkit aims at combining the shared intentions of customers and data analysts; they both seek to explore and understand data alike. Thus, this script investigates the inherent data structure to predicate the impact of the findings on the overall validation and matching outcome. It has been developed and tested to suit German customer data in particular. A fair share of German customer data still revolves around name and address entities. Thus, it pays special attention to particularities of the German language, namely compound words. I.e. the German counterpart to "legal department" is "Rechtsabteilung", a concatenated word. Compound words pose a bit of a challenge when it comes to token analysis. Let's stay with this example and assume "department", "Abteilung" in German - is a keyword used in the analysis step of the matching component. In fact, "Abteilung" will in most cases be part of a larger compound word rather than a standalone, which makes it harder to detect. In order to discover such occurrences, the script will follow a few steps:

1. load data into R
2. convert everything to lower case and replace special characters in data with a blank (you would be surprised how often you'd find "+", "/" as a name separator, especially in legacy systems)
3. split words on whitespace into single tokens
4. prepare list A containing all tokens of character length 2 and more (to include abbrev. & roman numerals)
5. prepare data frame B including all tokens that pass a certain threshold (token frequency and character length)
6. store list A in elasticsearch
7. query elasticsearch with tokens from data frame B to detect keywords hidden in compound words and collect examples for each search query (works just like a grep)
8. assign percentages to each token relatively to its overall occurrence
9. display results in sortable table

A few supporting technologies have been added to the bundle: elasticsearch was chosen over a classical grep, simply because it is more handy to search or add some nice visualizations somewhere down the line. A more prominent role is given to Jupyter Notebooks. The Jupyter Notebook is a powerful application that allows creating and sharing documents that contain live code, visualizations and explanatory text with customers and peers. Docker Container provide a great means of transporting the required infrastructure (Anaconda distribution, Jupyter Notebooks, R and elasticsearch).
